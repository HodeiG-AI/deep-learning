{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54320198-8842-4cc2-a9bd-be20c4d6af27",
   "metadata": {},
   "source": [
    "# Neural Learning: Gradient descent\n",
    "\n",
    "Below we have the main algoright to *predict, compare and learn*.\n",
    "\n",
    "- **epoch**: An epoch means training the neural network with all the training data for one cycle.\n",
    "- **weight**: The initial random weight of the neuron for a single input.\n",
    "- **goal_pred**: The true/target value to be predicted.\n",
    "- **input**: The input data to be used to predict the target.\n",
    "- **alpha**: Learning rate is used to prevent overcorrecting weight updates. The solution is to multiply the weight update by a\n",
    "fraction to make it smaller. Finding the appropriate alpha, even for state-of-the-art neural networks, is often done\n",
    "by guessing.\n",
    "\n",
    "For each epoch, the algorigthm will evaluate the following:\n",
    "1. **Predict**: It will make a prediction using the input and the weight. As the weight is randomly initialised it is very unlikely the prediction will be correct during the first epoch.\n",
    "2. **Compare**: The Mean Squared Error is used to calculate the error. MSE is a Performance Function but a different one could have been used. The MSE will be a positive number and it will amplify big errors and reduce small errors.\n",
    "3. **Learn**: At this step we need to update the weight. In order to do this we need to guess the direction that we want to move the weight and for how much (formally this is called the derivative). The direction is calculated as the *pure error* (pred - goal_pred) and it gets multiplied by the input for scaling, negative reversal and stopping. The derivative is eventually multiplied by the learning rate (alpha) to avoid overcorrecting.\n",
    "\n",
    "    3.1. **Direction**: In order to know the direction we can think of the following. If the true value was 1 and prediction was 0.6, we want to update the weight and if the true value was 0 reduce it. So for instance if the goal_pred > pred (1 > 0.6) the direction would be positive and we would increase the weight. Notice that in the below code the opposite is calculated (pred - goal_pred = pred > goald_pred) and therefore the weight is substracted using the weight delta rather than added. This means that if we calculate the weight delta as (pred - goal_pred) the weight needs to be substracted (weight -= weight_delta), but if (goal_pred - pred) is used the weight must be summed (weight += weight_delta).\n",
    "    \n",
    "    3.2. **Amount**: The pure error is multiplied by the input for these reasons:\n",
    "        - Stopping: If input is 0, then there is nothing to learn. It basically kills this neuron.\n",
    "        - Negative reversal: Multiplying pure error by input will flip the sign if input is negative\n",
    "        - Scaling: If input is big, weight update should also be big. Can go out of control. (Use alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58bcb50c-fd4b-4359-91c8-6764d8eec5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.040000\tPrediction: 1.0000\tDerivative: 0.4000\tWeight Delta: 0.0400\n",
      "Error: 0.014400\tPrediction: 0.9200\tDerivative: 0.2400\tWeight Delta: 0.0240\n",
      "Error: 0.005184\tPrediction: 0.8720\tDerivative: 0.1440\tWeight Delta: 0.0144\n",
      "Error: 0.001866\tPrediction: 0.8432\tDerivative: 0.0864\tWeight Delta: 0.0086\n",
      "Error: 0.000672\tPrediction: 0.8259\tDerivative: 0.0518\tWeight Delta: 0.0052\n",
      "Error: 0.000242\tPrediction: 0.8156\tDerivative: 0.0311\tWeight Delta: 0.0031\n",
      "Error: 0.000087\tPrediction: 0.8093\tDerivative: 0.0187\tWeight Delta: 0.0019\n",
      "Error: 0.000031\tPrediction: 0.8056\tDerivative: 0.0112\tWeight Delta: 0.0011\n",
      "Error: 0.000011\tPrediction: 0.8034\tDerivative: 0.0067\tWeight Delta: 0.0007\n",
      "Error: 0.000004\tPrediction: 0.8020\tDerivative: 0.0040\tWeight Delta: 0.0004\n",
      "Error: 0.000001\tPrediction: 0.8012\tDerivative: 0.0024\tWeight Delta: 0.0002\n",
      "Error: 0.000001\tPrediction: 0.8007\tDerivative: 0.0015\tWeight Delta: 0.0001\n",
      "Error: 0.000000\tPrediction: 0.8004\tDerivative: 0.0009\tWeight Delta: 0.0001\n",
      "Error: 0.000000\tPrediction: 0.8003\tDerivative: 0.0005\tWeight Delta: 0.0001\n",
      "Error: 0.000000\tPrediction: 0.8002\tDerivative: 0.0003\tWeight Delta: 0.0000\n",
      "Error: 0.000000\tPrediction: 0.8001\tDerivative: 0.0002\tWeight Delta: 0.0000\n",
      "Error: 0.000000\tPrediction: 0.8001\tDerivative: 0.0001\tWeight Delta: 0.0000\n",
      "Error: 0.000000\tPrediction: 0.8000\tDerivative: 0.0001\tWeight Delta: 0.0000\n",
      "Error: 0.000000\tPrediction: 0.8000\tDerivative: 0.0000\tWeight Delta: 0.0000\n",
      "Error: 0.000000\tPrediction: 0.8000\tDerivative: 0.0000\tWeight Delta: 0.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "weight = 0.5\n",
    "goal_pred = 0.8\n",
    "input = 2\n",
    "alpha = 0.1\n",
    "for epoch in range(epochs):\n",
    "    pred = input * weight\n",
    "    error = (pred - goal_pred) ** 2\n",
    "    derivative = input * (pred - goal_pred)\n",
    "    weight_delta = alpha * derivative\n",
    "    weight -= weight_delta\n",
    "    print(\"Error: %.6f\\tPrediction: %.4f\\tDerivative: %.4f\\tWeight Delta: %.4f\"\n",
    "          % (error, pred, derivative, weight_delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5082f09-6370-4e8d-877f-5db36fd0a6d2",
   "metadata": {},
   "source": [
    "The below example is the same as the previous one but *pure error* is calculated in the opposite way and therefore the weight gets updated summing the weight_delta, rather than decreasing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac047dc-6ef0-4edf-9408-9becef28fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.040000\tPrediction: 1.0000\tDerivative: -0.4000\tWeight Delta: -0.0400\n",
      "Error: 0.014400\tPrediction: 0.9200\tDerivative: -0.2400\tWeight Delta: -0.0240\n",
      "Error: 0.005184\tPrediction: 0.8720\tDerivative: -0.1440\tWeight Delta: -0.0144\n",
      "Error: 0.001866\tPrediction: 0.8432\tDerivative: -0.0864\tWeight Delta: -0.0086\n",
      "Error: 0.000672\tPrediction: 0.8259\tDerivative: -0.0518\tWeight Delta: -0.0052\n",
      "Error: 0.000242\tPrediction: 0.8156\tDerivative: -0.0311\tWeight Delta: -0.0031\n",
      "Error: 0.000087\tPrediction: 0.8093\tDerivative: -0.0187\tWeight Delta: -0.0019\n",
      "Error: 0.000031\tPrediction: 0.8056\tDerivative: -0.0112\tWeight Delta: -0.0011\n",
      "Error: 0.000011\tPrediction: 0.8034\tDerivative: -0.0067\tWeight Delta: -0.0007\n",
      "Error: 0.000004\tPrediction: 0.8020\tDerivative: -0.0040\tWeight Delta: -0.0004\n",
      "Error: 0.000001\tPrediction: 0.8012\tDerivative: -0.0024\tWeight Delta: -0.0002\n",
      "Error: 0.000001\tPrediction: 0.8007\tDerivative: -0.0015\tWeight Delta: -0.0001\n",
      "Error: 0.000000\tPrediction: 0.8004\tDerivative: -0.0009\tWeight Delta: -0.0001\n",
      "Error: 0.000000\tPrediction: 0.8003\tDerivative: -0.0005\tWeight Delta: -0.0001\n",
      "Error: 0.000000\tPrediction: 0.8002\tDerivative: -0.0003\tWeight Delta: -0.0000\n",
      "Error: 0.000000\tPrediction: 0.8001\tDerivative: -0.0002\tWeight Delta: -0.0000\n",
      "Error: 0.000000\tPrediction: 0.8001\tDerivative: -0.0001\tWeight Delta: -0.0000\n",
      "Error: 0.000000\tPrediction: 0.8000\tDerivative: -0.0001\tWeight Delta: -0.0000\n",
      "Error: 0.000000\tPrediction: 0.8000\tDerivative: -0.0000\tWeight Delta: -0.0000\n",
      "Error: 0.000000\tPrediction: 0.8000\tDerivative: -0.0000\tWeight Delta: -0.0000\n"
     ]
    }
   ],
   "source": [
    "weight = 0.5\n",
    "goal_pred = 0.8\n",
    "input = 2\n",
    "alpha = 0.1\n",
    "for iteration in range(20):\n",
    "    pred = input * weight\n",
    "    error = (pred - goal_pred) ** 2\n",
    "    derivative = input * (goal_pred - pred)\n",
    "    weight_delta = alpha * derivative\n",
    "    weight += weight_delta\n",
    "    print(\"Error: %.6f\\tPrediction: %.4f\\tDerivative: %.4f\\tWeight Delta: %.4f\"\n",
    "          % (error, pred, derivative, weight_delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00eaa9a-576c-4299-8230-fe7aecb5967e",
   "metadata": {},
   "source": [
    "# Neural Learning: Learning the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caffe82a-2050-4dd1-9f70-40fd4a88e206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [0 1 2 1]\n",
      "b: [2 2 2 3]\n",
      "c: [[0 2]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [1 3]]\n",
      "a * b: [0 2 4 3]\n",
      "a + b: [2 3 4 4]\n",
      "a * 0.5: [0.  0.5 1.  0.5]\n",
      "a + 0.5: [0.5 1.5 2.5 1.5]\n",
      "a . b: 9\n",
      "a . c: [6 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([0,1,2,1])\n",
    "b = np.array([2,2,2,3])\n",
    "c = np.array([[0,1,2,1], [2,2,2,3]]).T\n",
    "print(\"a: %s\" % a)\n",
    "print(\"b: %s\" % b)\n",
    "print(\"c: %s\" % c)\n",
    "\n",
    "print(\"a * b: %s\" % (a*b)) #elementwise multiplication\n",
    "print(\"a + b: %s\" % (a+b)) #elementwise addition\n",
    "print(\"a * 0.5: %s\" % (a * 0.5)) # vector-scalar multiplication\n",
    "print(\"a + 0.5: %s\" % (a + 0.5)) # vector-scalar addition\n",
    "print(\"a . b: %s\" % (a.dot(b)))\n",
    "print(\"a . c: %s\" % (a.dot(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7647846b-b708-484b-a737-c548f19f2e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0] Predictions: [-0.2    -0.2    -0.56    0.616   0.1728  0.1755]\n",
      "[Epoch: 0] Error: 2.6561\n",
      "\n",
      "[Epoch: 1] Predictions: [ 0.1404  0.3066 -0.3451  1.0066  0.4785  0.267 ]\n",
      "[Epoch: 1] Error: 0.9629\n",
      "\n",
      "[Epoch: 2] Predictions: [ 0.2136  0.5347 -0.2607  1.1319  0.6275  0.2543]\n",
      "[Epoch: 2] Error: 0.5509\n",
      "\n",
      "[Epoch: 3] Predictions: [ 0.2035  0.6562 -0.2219  1.1663  0.7139  0.2147]\n",
      "[Epoch: 3] Error: 0.3645\n",
      "\n",
      "[Epoch: 4] Predictions: [ 0.1718  0.7325 -0.1997  1.1698  0.772   0.173 ]\n",
      "[Epoch: 4] Error: 0.2517\n",
      "\n",
      "[Epoch: 5] Predictions: [ 0.1384  0.7865 -0.1837  1.1632  0.8149  0.1363]\n",
      "[Epoch: 5] Error: 0.1780\n",
      "\n",
      "[Epoch: 6] Predictions: [ 0.109   0.8274 -0.1704  1.1538  0.8482  0.1059]\n",
      "[Epoch: 6] Error: 0.1286\n",
      "\n",
      "[Epoch: 7] Predictions: [ 0.0848  0.8595 -0.1586  1.1438  0.8747  0.0815]\n",
      "[Epoch: 7] Error: 0.0951\n",
      "\n",
      "[Epoch: 8] Predictions: [ 0.0652  0.8851 -0.1477  1.1342  0.896   0.062 ]\n",
      "[Epoch: 8] Error: 0.0719\n",
      "\n",
      "[Epoch: 9] Predictions: [ 0.0496  0.9056 -0.1377  1.1251  0.9133  0.0465]\n",
      "[Epoch: 9] Error: 0.0556\n",
      "\n",
      "[Epoch: 10] Predictions: [ 0.0372  0.9222 -0.1283  1.1165  0.9273  0.0344]\n",
      "[Epoch: 10] Error: 0.0439\n",
      "\n",
      "[Epoch: 11] Predictions: [ 0.0275  0.9357 -0.1196  1.1086  0.9388  0.0248]\n",
      "[Epoch: 11] Error: 0.0354\n",
      "\n",
      "[Epoch: 12] Predictions: [ 0.0198  0.9466 -0.1115  1.1012  0.9482  0.0173]\n",
      "[Epoch: 12] Error: 0.0289\n",
      "\n",
      "[Epoch: 13] Predictions: [ 0.0139  0.9554 -0.104   1.0943  0.9559  0.0115]\n",
      "[Epoch: 13] Error: 0.0240\n",
      "\n",
      "[Epoch: 14] Predictions: [ 0.0092  0.9626 -0.0969  1.0878  0.9622  0.007 ]\n",
      "[Epoch: 14] Error: 0.0201\n",
      "\n",
      "[Epoch: 15] Predictions: [ 0.0056  0.9685 -0.0904  1.0818  0.9675  0.0035]\n",
      "[Epoch: 15] Error: 0.0170\n",
      "\n",
      "[Epoch: 16] Predictions: [ 2.8000e-03  9.7340e-01 -8.4200e-02  1.0763e+00  9.7190e-01  9.0000e-04]\n",
      "[Epoch: 16] Error: 0.0144\n",
      "\n",
      "[Epoch: 17] Predictions: [ 7.0000e-04  9.7730e-01 -7.8500e-02  1.0711e+00  9.7550e-01 -1.1000e-03]\n",
      "[Epoch: 17] Error: 0.0123\n",
      "\n",
      "[Epoch: 18] Predictions: [-8.0000e-04  9.8060e-01 -7.3200e-02  1.0662e+00  9.7850e-01 -2.5000e-03]\n",
      "[Epoch: 18] Error: 0.0106\n",
      "\n",
      "[Epoch: 19] Predictions: [-0.002   0.9833 -0.0682  1.0617  0.9811 -0.0036]\n",
      "[Epoch: 19] Error: 0.0091\n",
      "\n",
      "[Epoch: 20] Predictions: [-0.0029  0.9855 -0.0636  1.0575  0.9833 -0.0043]\n",
      "[Epoch: 20] Error: 0.0079\n",
      "\n",
      "[Epoch: 21] Predictions: [-0.0035  0.9874 -0.0593  1.0536  0.9851 -0.0048]\n",
      "[Epoch: 21] Error: 0.0068\n",
      "\n",
      "[Epoch: 22] Predictions: [-0.0038  0.989  -0.0552  1.05    0.9867 -0.0051]\n",
      "[Epoch: 22] Error: 0.0059\n",
      "\n",
      "[Epoch: 23] Predictions: [-0.0041  0.9903 -0.0515  1.0466  0.9881 -0.0053]\n",
      "[Epoch: 23] Error: 0.0051\n",
      "\n",
      "[Epoch: 24] Predictions: [-0.0042  0.9914 -0.048   1.0434  0.9892 -0.0053]\n",
      "[Epoch: 24] Error: 0.0044\n",
      "\n",
      "[Epoch: 25] Predictions: [-0.0043  0.9923 -0.0447  1.0405  0.9903 -0.0053]\n",
      "[Epoch: 25] Error: 0.0038\n",
      "\n",
      "[Epoch: 26] Predictions: [-0.0042  0.9932 -0.0417  1.0377  0.9912 -0.0052]\n",
      "[Epoch: 26] Error: 0.0033\n",
      "\n",
      "[Epoch: 27] Predictions: [-0.0041  0.9939 -0.0389  1.0351  0.9919 -0.005 ]\n",
      "[Epoch: 27] Error: 0.0029\n",
      "\n",
      "[Epoch: 28] Predictions: [-0.004   0.9945 -0.0362  1.0328  0.9926 -0.0049]\n",
      "[Epoch: 28] Error: 0.0025\n",
      "\n",
      "[Epoch: 29] Predictions: [-0.0039  0.995  -0.0338  1.0305  0.9933 -0.0047]\n",
      "[Epoch: 29] Error: 0.0022\n",
      "\n",
      "[Epoch: 30] Predictions: [-0.0037  0.9954 -0.0315  1.0285  0.9938 -0.0045]\n",
      "[Epoch: 30] Error: 0.0019\n",
      "\n",
      "[Epoch: 31] Predictions: [-0.0036  0.9959 -0.0293  1.0265  0.9943 -0.0042]\n",
      "[Epoch: 31] Error: 0.0016\n",
      "\n",
      "[Epoch: 32] Predictions: [-0.0034  0.9962 -0.0274  1.0247  0.9948 -0.004 ]\n",
      "[Epoch: 32] Error: 0.0014\n",
      "\n",
      "[Epoch: 33] Predictions: [-0.0032  0.9965 -0.0255  1.023   0.9952 -0.0038]\n",
      "[Epoch: 33] Error: 0.0012\n",
      "\n",
      "[Epoch: 34] Predictions: [-0.003   0.9968 -0.0238  1.0215  0.9955 -0.0036]\n",
      "[Epoch: 34] Error: 0.0011\n",
      "\n",
      "[Epoch: 35] Predictions: [-0.0029  0.9971 -0.0222  1.02    0.9959 -0.0034]\n",
      "[Epoch: 35] Error: 0.0009\n",
      "\n",
      "[Epoch: 36] Predictions: [-0.0027  0.9973 -0.0206  1.0187  0.9962 -0.0032]\n",
      "[Epoch: 36] Error: 0.0008\n",
      "\n",
      "[Epoch: 37] Predictions: [-0.0025  0.9975 -0.0192  1.0174  0.9965 -0.003 ]\n",
      "[Epoch: 37] Error: 0.0007\n",
      "\n",
      "[Epoch: 38] Predictions: [-0.0024  0.9977 -0.0179  1.0162  0.9967 -0.0028]\n",
      "[Epoch: 38] Error: 0.0006\n",
      "\n",
      "[Epoch: 39] Predictions: [-0.0022  0.9979 -0.0167  1.0151  0.9969 -0.0026]\n",
      "[Epoch: 39] Error: 0.0005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "weights = np.array([0.5,0.48,-0.7])\n",
    "alpha = 0.1\n",
    "epochs = 40\n",
    "\n",
    "streetlights = np.array( [[ 1, 0, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 0, 0, 1 ],\n",
    "                          [ 1, 1, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 1, 0, 1 ] ] )\n",
    "\n",
    "walk_vs_stop = np.array( [ 0, 1, 0, 1, 1, 0 ] )\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    error_for_all_lights = 0\n",
    "    predictions = []\n",
    "    for row_index in range(len(walk_vs_stop)):\n",
    "        input = streetlights[row_index]\n",
    "        goal_prediction = walk_vs_stop[row_index]\n",
    "        \n",
    "        prediction = input.dot(weights)\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        error = (goal_prediction - prediction) ** 2\n",
    "        error_for_all_lights += error\n",
    "        \n",
    "        delta = prediction - goal_prediction\n",
    "        weights = weights - (alpha * (input * delta))\n",
    "    print(\"[Epoch: %s] Predictions: %s\" % (epoch, np.round( [float(i) for i in predictions], 4)))\n",
    "    print(\"[Epoch: %s] Error: %.4f\\n\" % (epoch, error_for_all_lights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa6d07-e58e-4030-9f23-cb218284dc3b",
   "metadata": {},
   "source": [
    "# Neural Learning: Backpropagation\n",
    "\n",
    "## [ReLu](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks) (Rectified linear activation unit)\n",
    "\n",
    "In order to use stochastic gradient descent with backpropagation of errors to train deep neural networks, an activation function is needed that looks and acts like a linear function, but is, in fact, a nonlinear function allowing complex relationships in the data to be learned.\n",
    "\n",
    "Nonlinear activation functions are preferred as they allow the nodes to learn more complex structures in the data. Traditionally, two widely used nonlinear activation functions are the sigmoid and hyperbolic tangent activation functions.\n",
    "\n",
    "The sigmoid activation function, also called the logistic function, is traditionally a very popular activation function for neural networks. The input to the function is transformed into a value between 0.0 and 1.0. Inputs that are much larger than 1.0 are transformed to the value 1.0, similarly, values much smaller than 0.0 are snapped to 0.0. The shape of the function for all possible inputs is an S-shape from zero up through 0.5 to 1.0. For a long time, through the early 1990s, it was the default activation used on neural networks.\n",
    "\n",
    "The hyperbolic tangent function, or tanh for short, is a similar shaped nonlinear activation function that outputs values between -1.0 and 1.0. In the later 1990s and through the 2000s, the tanh function was preferred over the sigmoid activation function as models that used it were easier to train and often had better predictive performance.\n",
    "\n",
    "The solution had been bouncing around in the field for some time, although was not highlighted until papers in 2009 and 2011 shone a light on it. The solution is to use the rectified linear activation function, or ReL for short. A node or unit that implements this activation function is referred to as a rectified linear activation unit, or ReLU for short. Often, networks that use the rectifier function for the hidden layers are referred to as rectified networks. Adoption of ReLU may easily be considered one of the few milestones in the deep learning revolution, e.g. the techniques that now permit the routine development of very deep neural networks.\n",
    "\n",
    "For a long time, the default activation to use was the sigmoid activation function. Later, it was the tanh activation function. For modern deep learning neural networks, the default activation function is the rectified linear activation function.\n",
    "\n",
    "It is recommended as the default for both Multilayer Perceptron (MLP) and Convolutional Neural Networks (CNNs). Given their careful design, ReLU were thought to not be appropriate for Recurrent Neural Networks (RNNs) such as the Long Short-Term Memory Network (LSTM) by default. When using ReLU in your network, consider setting the bias to a small value, such as 0.1. Before training a neural network,the weights of the network must be initialized to small random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "58b97619-9846-49b4-bac4-3292b2790772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights\n",
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
      " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
      "[[-0.5910955 ]\n",
      " [ 0.75623487]\n",
      " [-0.94522481]\n",
      " [ 0.34093502]]\n",
      "\n",
      "Start\n",
      "[Epoch: 9] Predictions: [0.8478 0.4052 0.4261 0.2751]\n",
      "[Epoch: 9] Error: 0.634231\n",
      "\n",
      "[Epoch: 19] Predictions: [0.9919 0.5718 0.3627 0.2084]\n",
      "[Epoch: 19] Error: 0.358384\n",
      "\n",
      "[Epoch: 29] Predictions: [1.     0.8191 0.2162 0.0598]\n",
      "[Epoch: 29] Error: 0.083018\n",
      "\n",
      "[Epoch: 39] Predictions: [1.     0.9598 0.0695 0.0043]\n",
      "[Epoch: 39] Error: 0.006467\n",
      "\n",
      "[Epoch: 49] Predictions: [1.     0.9921 0.0163 0.    ]\n",
      "[Epoch: 49] Error: 0.000329\n",
      "\n",
      "[Epoch: 59] Predictions: [1.     0.9983 0.0035 0.    ]\n",
      "[Epoch: 59] Error: 0.000015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0) * x # returns x if x > 0\n",
    "                       # return 0 otherwise\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output>0 # returns 1 for input > 0\n",
    "                    # return 0 otherwise\n",
    "epochs = 60\n",
    "alpha = 0.2\n",
    "hidden_size = 4\n",
    "\n",
    "streetlights = np.array( [[ 1, 0, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 0, 0, 1 ],\n",
    "                          [ 1, 1, 1 ] ] )\n",
    "walk_vs_stop = np.array([[ 1, 1, 0, 0]]).T\n",
    "\n",
    "weights_0_1 = 2*np.random.random((3,hidden_size)) - 1  # Create random values between -1 and 1\n",
    "weights_1_2 = 2*np.random.random((hidden_size,1)) - 1  # Create random values between -1 and 1\n",
    "\n",
    "print(\"Weights\")\n",
    "print(weights_0_1)\n",
    "print(weights_1_2)\n",
    "\n",
    "print(\"\\nStart\")\n",
    "for epoch in range(epochs):\n",
    "   layer_2_error = 0\n",
    "   predictions = []\n",
    "   for i in range(len(streetlights)):\n",
    "      layer_0 = streetlights[i:i+1]\n",
    "      layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "      layer_2 = np.dot(layer_1,weights_1_2)\n",
    "      predictions.append(layer_2[0][0])\n",
    "\n",
    "      layer_2_error += np.sum((layer_2 - walk_vs_stop[i:i+1]) ** 2)\n",
    "\n",
    "      layer_2_delta = (walk_vs_stop[i:i+1] - layer_2)\n",
    "      layer_1_delta=layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1)\n",
    "\n",
    "      weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "      weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "   if(epoch % 10 == 9):\n",
    "      print(\"[Epoch: %s] Predictions: %s\" % (epoch, np.round( [float(i) for i in predictions], 4)))\n",
    "      print(\"[Epoch: %s] Error: %.6f\\n\" % (epoch, layer_2_error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
